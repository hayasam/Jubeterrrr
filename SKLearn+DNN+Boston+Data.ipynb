{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SKLearn Tutorial: DNN on Boston Data</h1>\n",
    "<br />\n",
    "This tutorial follows very closely two other good tutorials and merges elements from both:\n",
    "<ul>\n",
    "<li><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/boston.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/boston.py</a> \n",
    "<li><a href=\"http://bigdataexaminer.com/uncategorized/how-to-run-linear-regression-in-python-scikit-learn/\">http://bigdataexaminer.com/uncategorized/how-to-run-linear-regression-in-python-scikit-learn/</a>\n",
    "\n",
    "</ul>\n",
    "<br />\n",
    "D. Thiebaut<br />\n",
    "August 2016.\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Get the Boston Data</h2>\n",
    "This part is basically taken directly from the <a href=\"http://bigdataexaminer.com/uncategorized/how-to-run-linear-regression-in-python-scikit-learn/\">bigdataexaminer</a> tutorial.\n",
    "\n",
    "<br />\n",
    "Imports, first!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "#from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import learn\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import explained_variance_score,mean_absolute_error,median_absolute_error\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "street = pd.read_csv('Datatotalx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convert the boston data into a panda data-frame</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = street.drop('y',axis=1)\n",
    "y_val = street['y']\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_train = list(street.columns)\n",
    "col_train_bis = list(street.columns)\n",
    "col_train_bis.remove('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col=list(x_data.columns)\n",
    "FEATURES=feature_col\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_val,test_size=0.3,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " scaler.fit( X_train )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the X data to 0 mean and unit standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " scaler.fit( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train=np.matrix(X_train)\n",
    "y_train=np.array(y_train)\n",
    "X_test=np.matrix(X_test)\n",
    "y_test=np.array(y_test)\n",
    "y_train=y_train.reshape(-1,y_train.size)\n",
    "y_test=y_test.reshape(-1,y_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building a [10,10] Neural Net</h1>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(data=scaler.transform(X_train),columns = X_train.columns,index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(data=scaler.transform(X_test),columns = X_test.columns,index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_func = tf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train ,batch_size=10,num_epochs=1000,\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti = tf.train.AdamOptimizer(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\hp\\AppData\\Local\\Temp\\tmpj04vlaih\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027815E4C358>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Temp\\\\tmpj04vlaih'}\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'x1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-240-7702bfc4209e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNNRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopti\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m                 instructions)\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[0m_verify_input_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m       \u001b[0mSKCompat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, steps, max_steps, monitors)\u001b[0m\n\u001b[0;32m   1525\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1527\u001b[1;33m         monitors=all_monitors)\n\u001b[0m\u001b[0;32m   1528\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m                 instructions)\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[0;32m    522\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks)\u001b[0m\n\u001b[0;32m   1039\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m       \u001b[0mtraining_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_or_create_global_step_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m       \u001b[0mmodel_fn_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLOSSES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m       \u001b[0mall_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_get_train_ops\u001b[1;34m(self, features, labels)\u001b[0m\n\u001b[0;32m   1262\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \"\"\"\n\u001b[1;32m-> 1264\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_eval_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[1;34m(self, features, labels, mode, metrics, config)\u001b[0m\n\u001b[0;32m   1225\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'model_dir'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_dir'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m     \u001b[0mmodel_fn_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn.py\u001b[0m in \u001b[0;36m_dnn_model_fn\u001b[1;34m(features, labels, mode, params, config)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mfeature_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mweight_collections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparent_scope\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             scope=input_layer_scope)\n\u001b[0m\u001b[0;32m    162\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         net = fc_core.input_layer(\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\u001b[0m in \u001b[0;36minput_from_feature_columns\u001b[1;34m(columns_to_tensors, feature_columns, weight_collections, trainable, scope, cols_to_outs)\u001b[0m\n\u001b[0;32m    219\u001b[0m                                      \u001b[0moutput_rank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                                      \u001b[0mdefault_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'input_from_feature_columns'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m                                      cols_to_outs=cols_to_outs)\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\u001b[0m in \u001b[0;36m_input_from_feature_columns\u001b[1;34m(columns_to_tensors, feature_columns, weight_collections, trainable, scope, output_rank, default_name, cols_to_outs)\u001b[0m\n\u001b[0;32m    118\u001b[0m                                          \u001b[0mdefault_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                                          values=columns_to_tensors.values()):\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mtransformed_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_rank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m           transformed_tensor = nest.map_structure(\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, feature_column)\u001b[0m\n\u001b[0;32m    846\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_columns_to_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m     \u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert_transformed_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_columns_to_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfeature_column\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_columns_to_tensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda33\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column.py\u001b[0m in \u001b[0;36minsert_transformed_feature\u001b[1;34m(self, columns_to_tensors)\u001b[0m\n\u001b[0;32m   1869\u001b[0m     \"\"\"\n\u001b[0;32m   1870\u001b[0m     \u001b[1;31m# Transform the input tensor according to the normalizer function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1871\u001b[1;33m     \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_normalized_input_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns_to_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1872\u001b[0m     \u001b[0mcolumns_to_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'x1'"
     ]
    }
   ],
   "source": [
    "# Build a 2 layer fully connected DNN\n",
    "\n",
    "regressor=tf.contrib.learn.DNNRegressor(hidden_units=[11,11],feature_columns=feature_cols,optimizer=opti, dropout=0.2)\n",
    "regressor.fit( X_train, y_train, steps=5000, batch_size=1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " y_predicted = regressor.predict( scaler.transform( X_test ) )\n",
    " score = metrics.mean_squared_error( y_predicted, y_test )\n",
    " print('MSE: {0:f}'.format(score) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter( y_predicted, y_test, s=5 )\n",
    "plt.xlabel( \"Predicted Prices\")\n",
    "plt.ylabel( \"Real Prices\")\n",
    "plt.title( \"Real vs Predicted Housing Prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generating the graph of the NN with tensorboard</h1>\n",
    "<br />\n",
    "In order to generate the graph of the network, we simply add \"model_dir = '/tmp/tf' \" to the DNNRegressor() instantiation:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "regressor = learn.DNNRegressor( feature_columns=None,\n",
    "                                hidden_units=[10, 10] ),\n",
    "                                model_dir = '/tmp/tf')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might generate an error when we try to fit, but that will happen after the useful information is written in /tmp/tf, and we can safely run tensorboard as follows, from the command line:\n",
    "\n",
    "<pre>\n",
    "tensorboard --logdir /tmp/tf\n",
    "</pre>\n",
    "<br />\n",
    "And here's the graph we get for the 10x10 NN:\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"files/DNNRegressor1.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building a [13,13] Neural Net</h1>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a 2 layer fully connected DNN\n",
    "regressor = learn.DNNRegressor( feature_columns=None,\n",
    "                               hidden_units=[13, 13])\n",
    "regressor.fit(X_train, y_train, steps=5000, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " y_test_predicted = regressor.predict( scaler.transform( X_test ) )\n",
    " scoreTest = metrics.mean_squared_error( y_test_predicted, y_test )\n",
    " print('MSE Test Data: {0:f}'.format(scoreTest) )\n",
    "\n",
    " y_train_predicted = regressor.predict( X_train  )\n",
    " scoreTrain = metrics.mean_squared_error( y_train_predicted, y_train )\n",
    " print('MSE Train Data: {0:f}'.format(scoreTrain ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter( y_predicted, y_test, s=5 )\n",
    "plt.xlabel( \"Predicted Prices\")\n",
    "plt.ylabel( \"Real Prices\")\n",
    "plt.title( \"Real vs Predicted Housing Prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building a [13,13,10] Neural Net</h1>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a 2 layer fully connected DNN\n",
    "feature_columns = boston.feature_names\n",
    "regressor = learn.DNNRegressor( feature_columns=None,\n",
    "                               hidden_units=[13, 13, 10],\n",
    "                              model_dir = '/tmp/tf/')\n",
    "regressor.fit(X_train, y_train, steps=5000, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " y_test_predicted = regressor.predict( scaler.transform( X_test ) )\n",
    " scoreTest = metrics.mean_squared_error( y_test_predicted, y_test )\n",
    " print('MSE Test Data: {0:f}'.format(scoreTest) )\n",
    "\n",
    " y_train_predicted = regressor.predict( X_train  )\n",
    " scoreTrain = metrics.mean_squared_error( y_train_predicted, y_train )\n",
    " print('MSE Train Data: {0:f}'.format(scoreTrain ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter( y_predicted, y_test, s=5 )\n",
    "plt.xlabel( \"Predicted Prices\")\n",
    "plt.ylabel( \"Real Prices\")\n",
    "plt.title( \"Real vs Predicted Housing Prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Plotting Residuals</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( y_train_predicted, y_train_predicted - y_train,\n",
    "           c ='b', s=30, alpha=0.4 )\n",
    "plt.scatter(y_test_predicted, y_test_predicted - y_test,\n",
    "           c ='g', s=30 )\n",
    "plt.hlines( y=0, xmin=-5, xmax=55)\n",
    "plt.title( \"Residuals\" )\n",
    "plt.ylabel( \"Residuals\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
